{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>DATA_SCI 423 | Final Project</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Understanding Alloy Steel Composition-Property <br/> Relationships Using Machine Learning<center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Raymond Wang, Yangdongling Liu </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We investigate the composition-property relationships in alloy steels using machine learning models. Alloys that consist of multi-elemental composition exhibit a large variety of physical properties, and have been widely used in modern industry. For instance, copper alloys are commonly used in the manufacture of electrical equipment, titanium alloys are most often seem in biomaterial applications. However, the high dimensional composition space makes it challenging to understand how elemental composition govern alloy properties. Existing theoretical models such as finite element model (FEM) are in general computationally expensive. In this work, we present a reliable and efficient way to directly predict alloy steel properties from their elemental composition using machine learning.\n",
    "\n",
    "The workflow of this project is shown in the figure below. We obtain 855 alloy steel data from unstructured online resources. After data preprocessing, we first analyze correlation within the dataset, where several strongly correlated quantities are identified (e.g. hardness and tensile strength). Then we benchmark the performance of 10 machine learning algorithms. Extreme gradient boosting (XGBoost) outperforms the other models in our case. Further analysis shows that predicting thermal conductivity from chemical composition using XGBoost has satisfying accuracy and the model performance could be improved by having more training data. \n",
    "\n",
    "Our findings suggest that machine learning is a powerful tool that could provide more insights of alloy steel composition-property relationship than using human physical intuition or experience, and that data-driven research is a thriving division in physical science community."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"workflow.png\" alt=\"drawing\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Data Acquisition and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alloy steel data is collected froma [Matweb](http://www.matweb.com/index.aspx), where alloy steels containing Manganese, Chromium, and Nickel are set as the target materials for scraping. The scraping code automatically collects materials information that meets the searching criteria and saves to local CSV files. Physical properties (e.g. hardness, thermal conductivity) in different units, chemical compositions (e.g. per centage weight of Mn, Ni, Cr), and the potentially helpful comments, are saved to local files with unchanged format. Since there is no data-editing involved in this step, the local data is guaranteed to be the same as its online version. Eventually the program extracted 855 alloy steels that meet the criteria.\n",
    "\n",
    "Simple data-processing is performed right after all the data has been saved locally. This step modifies the file names and contents containing non-utf-8 encoding and fixes unwanted line breaks. The data collected from the last step is still string-based, e.g. '97%' is interpreted as a sequence of characters instead of a floating point number. Therefore, it is necessary to convert these strings to machine-readable format (i.e. floating point numbers) before any further data analysis.\n",
    "\n",
    "These physical variables include: \n",
    "\n",
    "|   |  \t|  \t|  \t|  \t|  \t|\n",
    "|:-------------------------------:|:---------------------:|:----------------------:|:------------------------:|:---------------:|:---------------------------:|\n",
    "|density|hardness (Vickers)|thermal conductivity|specific heat capacity|CTE-linear|poisson's ratio|\n",
    "|electrical resistivity|elongation at break|bulk modulus|modulus of elasticity|shear modulus|tensile strength at yield|\n",
    "\n",
    "    \n",
    "    \n",
    "Ten element types including are considered, including:\n",
    "\n",
    "| |  |  |  |  |  |  |  |  |  |\n",
    "|:--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|--------:|\n",
    "| Fe | Mn | Cr | Ni | Mo | Cu | C | S | Si | P |\n",
    "\n",
    "Some of the most significant code implemented in this section is shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import Select\n",
    "\n",
    "# navigate to target website\n",
    "options = webdriver.ChromeOptions()\n",
    "driver_path = os.getcwd() + \"/chromedriver-linux\"\n",
    "driver = webdriver.Chrome(executable_path=driver_path, chrome_options=options)\n",
    "driver.get(\"http://www.matweb.com/index.aspx\")\n",
    "...\n",
    "# navigate into each alloy page\n",
    "driver.find_element_by_link_text(name).click()\n",
    "table = driver.find_element_by_xpath(\"//table[@class='tabledataformat']\")\n",
    "attrib = []\n",
    "for row in table.find_elements_by_xpath(\"//tr[@class='altrow datarowSeparator']\"):\n",
    "    attrib.append([d.text for d in row.find_elements_by_css_selector('td')])\n",
    "    for row in table.find_elements_by_xpath(\"//tr[@class=' datarowSeparator']\"):\n",
    "        attrib.append([d.text for d in row.find_elements_by_css_selector('td')])\n",
    "attrib = np.array(attrib)\n",
    "...\n",
    "# collect features from files\n",
    "data_all_string = []\n",
    "for filename in os.listdir(ddir):\n",
    "    instance = ['0%']*len(elem_list) + ['NA']*len(prop_list) # initialize instance row\n",
    "    data = pd.read_csv(ddir+filename, header=0, index_col=0)\n",
    "    for ii in range(nfeature):\n",
    "        if feature_list[ii] in data.index:\n",
    "            instance[ii] = data.loc[feature_list[ii], 'Metric']\n",
    "            # get rid of some unexpected '\\n' in dataset\n",
    "            if '\\n' in instance[ii]: instance[ii] = instance[ii].replace('\\n', ' ') \n",
    "    if instance[0] is not 0: data_all_string.append(instance) # steel only\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of the dataset after featurization is 855$\\times$23 (with 855 instances and 23 features). It is quite impractical for humans to directly learn patterns from such a large amount of data. Let's take a look at what we have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import pandas as pd\n",
    "data = pd.read_csv('./data_all_float.csv', sep=';', header=0)\n",
    "data.head()\n",
    "\n",
    "   Iron, Fe  Carbon, C  Sulfur, S     ...     Shear Modulus  Poissons Ratio  Tensile Strength\n",
    "0  97.38575      0.300       0.04     ...              80.0            0.29           850.0          \n",
    "1  95.72250      0.405       0.02     ...              78.0            0.20           786.0           \n",
    "2  96.46250      0.200       0.04     ...              80.0            0.11           958.0           \n",
    "3  96.46250      0.200       0.04     ...              80.0            0.45           1125.0         \n",
    "4  97.38750      0.300       0.04     ...              80.0            0.10           425.0           \n",
    "...\n",
    "[855 rows x 23 columns]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using basic statistical knowledge, we decide to start from learning the correlation patten of these properties. The instances with 'NaN' entries are dropped from the dataset, and eventually 254 materials are used to generate the heatmap, as shown below. The upper left corner could be safely ignored since it does not contain useful composition-property information. The lower left part reveals the relationship between elements and different physical properties. The more 'read shift' the block is, the more positive correlation the element has to the property, and vice versa. The lower right region reflects how these physical properties themselves are correlated. This could potentially be interesting as well, but is out of the scope of this study.\n",
    "\n",
    "Some of the findings agree well with the way elements contribute to alloy steel properties as reported online, e.g. carbon decreases ductility of steel, and could lead to a small shear modulus. However, the information from correlation analysis is more qualitative than quantitative. If we want more quantitative descriptions of the materials composition-property relationship, more sophisticated methods are needed. In the next section we discuss alloy steel data analysis using machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "data = pd.read_csv(\"data_all_float.csv\", header=0, index_col=None, sep=';')\n",
    "# compute correlation\n",
    "corr = data.corr()\n",
    "\n",
    "# plot correlation heatmap\n",
    "fig = plt.figure(num=None, figsize=(40, 40), dpi=80, facecolor='w', edgecolor='w')\n",
    "colormap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "mask = np.zeros_like(corr)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "ylabel = list(corr.columns)\n",
    "ax = sns.heatmap(corr, cmap=colormap, mask=mask, annot=True, fmt=\".2f\", \n",
    "                 xticklabels=corr.columns[:-1], yticklabels=ylabel)\n",
    "plt.title(label=\"Correlation Heatmap of All Features\", loc='center', \n",
    "          fontdict={'fontname':'DejaVu Sans', 'size':'24', 'color':'black', \n",
    "                    'weight':'bold', 'verticalalignment':'bottom'})\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"CorrelationHeatmap.png\" alt=\"drawing\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Benchmarking Machine Learning Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform systematic benchmark of different machine learning algorithm performance on various physical property predictions.\n",
    "\n",
    "Each training takes one physical property (e.g. hardness) as the target, while the other 12 properties plus aforementioned 10 element types are used as input features. Instances with 'NaN' are dropped from the dataset. Both X and y are standardized (i.e. centered to zero and scaled to unit variance) as implemented in StandardScaler() from sklearn.\n",
    "\n",
    "10 machine learning algorithms are benchmarked here, including:\n",
    "- Linear Regression\t\n",
    "- Least-angle regression with Lasso (LassoLars)\n",
    "- Kernel Ridge\n",
    "- Linear Support Vector Regression (Linear SVR)\n",
    "- Stochastic Gradient Descent Regression (SGD)\n",
    "- Multi-layer Perceptron Regressor (MLP)\n",
    "- AdaBoost Regression\n",
    "- Random Forest Regression\n",
    "- Gradient Boosting Regression\n",
    "- Extreme Gradient Boosting (XGBoost)\n",
    "\n",
    "The dataset is divided into training (75%) and testing (25%) parts. Grid search method is used for hyper-parameter tuning. The set of hyper-parameters are shown in Supporting Information. The optimal setting of the parameters is determined based on 5-fold cross-validation performed on training data only. R2 score is used as error metric. \n",
    "\n",
    "The key part of our implementation is attached below (using XGBoost as an example):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import sys\n",
    "import xgboost\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import *\n",
    "\n",
    "# set hyper-parameters\n",
    "hyper_params = [{\n",
    "    'n_estimators' : (10, 50, 100, 250, 500, 1000,),\n",
    "    'learning_rate' : (0.0001,0.01, 0.05, 0.1, 0.2,),\n",
    "    'gamma' : (0,0.1,0.2,0.3,0.4,),\n",
    "    'max_depth' : (6,),\n",
    "    'subsample' : (0.5, 0.75, 1,),\n",
    "}]\n",
    "\n",
    "stdscale = StandardScaler()\n",
    "\n",
    "properties = [ 'Thermal Conductivity', 'Specific Heat Capacity', 'Hardness, Vickers', \n",
    "              'Electrical Resistivity', 'Elongation at Break', 'Bulk Modulus', \n",
    "              'Modulus of Elasticity', 'Shear Modulus', 'Poissons Ratio', \n",
    "              'Tensile Strength, Yield', 'Tensile Strength, Ultimate']\n",
    "\n",
    "for target in properties:\n",
    "    X = data.drop(target, axis=1).values\n",
    "    y = data[target].values\n",
    "    \n",
    "    # standardize features\n",
    "    X = stdscale.fit_transform(X)\n",
    "    y = stdscale.fit_transform(y.reshape(-1, 1)) \n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.75, \n",
    "                                                        test_size=0.25, random_state=0)\n",
    "    \n",
    "    regressor = xgboost.XGBRegressor()\n",
    "\n",
    "    grid_clf = GridSearchCV(regressor, cv=5, param_grid=hyper_params,\n",
    "                            verbose=1, n_jobs=8, scoring='r2')\n",
    "    grid_clf.fit(X_train, y_train.ravel())\n",
    "\n",
    "    train_score_mse = mean_squared_error(stdscale.inverse_transform(y_train),\n",
    "                                         stdscale.inverse_transform(grid_clf.predict(X_train)))\n",
    "    test_score_mse = mean_squared_error(stdscale.inverse_transform(y_test),\n",
    "                                        stdscale.inverse_transform(grid_clf.predict(X_test)))\n",
    "\n",
    "    sorted_grid_params = sorted(grid_clf.best_params_.items(), key=operator.itemgetter(0))\n",
    "\n",
    "    # print results\n",
    "    out_txt = '\\t'.join(['algorithm: ', str(sorted_grid_params).replace('\\n', ','), \n",
    "                         str(train_score_mse), str(test_score_mse)])\n",
    "\n",
    "    print(out_txt)\n",
    "    sys.stdout.flush()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are plotted below. Training errors are shown in circles and solid lines, test errors are in squares and dashed lines. The y-axis is the calculated root mean square error (RMSE) value divided by the difference of maximum and minimum value in the target data. This normalization step is essential so as to directly compare the performance of different machine learning algorithms in predicting quantities at various scales. The sklearn built-in performance scores (e.g. R2-score) are not used here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Benchmark.png\" alt=\"drawing\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, some trends can be identified across different algorithms as well as physical properties. In general, linear regression, Lasso, linear SVR and SGD algorithms lead to larger variance in normalized RMSE, while the rest have relatively better performance. XGBoost algorithm achieves the best performance among the 10 benchmarked methods.\n",
    "\n",
    "The physical properties are divided into two panels due to different scales of RMSE. We find it hard to have a good prediction in hardness and tensile strength, but it may not be a coincidence. It is exciting to notice that from the correlation heatmap analysis, it is clear that hardness is strongly and positively correlated to tensile strengths. This may suggest that these quantities are related to some other factors not considered here, such as processing and microscopic structures. And we need to include the other important features in order to build better machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. Learning Thermal Properties with XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We further investigate machine learning model performance on more challenging problems. In the previous section a single property is targeted using both alloy chemical composition and other known physical properties. However, in practical situations, sometimes other properties are also unknown (e.g. a new family of alloy steel). Therefore, our aim here is to further understand the composition-property relationship in alloy steel, i.e. we will only use elemental composition information as input features to predict physical properties. We choose to use Extreme Gradient Boosting (XGBoost) algorithm for further analysis on alloy steel composition-relationship since it outperforms the other algorithms from our benchmark results.\n",
    "\n",
    "The implementation is essentailly the same as shown previously in the benchmark section and is therefore not presented here. The performance of XGBoost in predicting 11 different physical properties independently below. R2-score is used as error metric in this case. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"XGBoostR2.png\" alt=\"drawing\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among the 11 tested properties, 8 of them do not show a strong relationship to composition, both training score and test score are below 0.5. Specific heat capacity and electrical resistivity get a high score in training but score poorly in test. This probably means XGBoost cannot correctly capture the relationship from the training set, but only numerically fit the data. In other words, the model suffers from overfitting. One thing to notice here is that R2-score can be negative, which means the model is even worse than a horizontal line through the mean feature value.\n",
    "\n",
    "XGBoost performs well in predicting thermal conductivity using composition information, where the test score is even higher than training score. In order to validate our findings, we check the learning curve of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import xgboost\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "stdscale = StandardScaler()\n",
    "X = stdscale.fit_transform(X)\n",
    "y = stdscale.fit_transform(y.reshape(-1, 1)) \n",
    "\n",
    "train_sizes, train_scores, valid_scores = learning_curve(xgboost.XGBRegressor(), X, y, \n",
    "                                                         train_sizes=list(range(20, 490, 10)), cv=5)\n",
    "out = []\n",
    "for ii in range(len(train_sizes)):\n",
    "    out.append([train_sizes[ii], np.mean(train_scores[ii]), np.mean(valid_scores[ii])])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning curve of predicting thermal conductivity is shown below. As the number of training samples increase, cross-validation scores quickly increase and converge to the high training score. This clearly means adding more training samples can further improve the performance of XGBoost model in predicting thermal conductivity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"XGBoostLearncurve.png\" alt=\"drawing\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VI. Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find that machine learning could provide both qualitative and quantitative insights of alloy steel composition-property relationship. Gradient boost-based algorithms outperform the other machine learning algorithms in our benchmark. XGBoost is particularly successful in predicting thermal conductivity using alloy chemical composition, and the performance can be improved by adding more training samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supporting Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>Table: Hyper-parameters used in different machine learning algorithms for grid search.<center/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"hyper_param.png\" alt=\"drawing\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P.S. Open to collaboration on data-driven research in materials science and engineering.<br/>\n",
    "Please contact <raymondwang@u.northwestern.edu> if interested, or visit us at our [group website](https://mtd.mccormick.northwestern.edu/research/)!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
